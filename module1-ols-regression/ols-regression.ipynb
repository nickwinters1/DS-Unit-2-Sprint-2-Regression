{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science_\n",
    "\n",
    "# Ordinary Least Squares Regression\n",
    "\n",
    "## What is Linear Regression?\n",
    "\n",
    "Linear Regression is a statistical model that seeks to describe the relationship between some y variable and one or more x variables. \n",
    "\n",
    "![Linear Regression](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png)\n",
    "\n",
    "In the simplest case, linear regression seeks to fit a straight line through a cloud of points. This line is referred to as the \"regression line\" or \"line of best fit.\" This line tries to summarize the relationship between our X and Y in a way that enables us to use the equation for that line to make predictions.\n",
    "\n",
    "### Synonyms for \"y variable\" \n",
    "- Dependent Variable\n",
    "- Response Variable\n",
    "- Outcome Variable \n",
    "- Predicted Variable\n",
    "- Measured Variable\n",
    "- Explained Variable\n",
    "- Label\n",
    "- Target\n",
    "\n",
    "### Synonyms for \"x variable\"\n",
    "- Independent Variable\n",
    "- Explanatory Variable\n",
    "- Regressor\n",
    "- Covariate\n",
    "- Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regresion (bivariate)\n",
    "\n",
    "## Making Predictions\n",
    "\n",
    "Say that we were trying to create a model that captured the relationship between temperature outside and ice cream sales. In Machine Learning our goal is often different that of other flavors of Linear Regression Analysis, because we're trying to fit a model to this data with the intention of making **predictions** on new data (in the future) that we don't have yet.\n",
    "\n",
    "## What are we trying to predict?\n",
    "\n",
    "So if we had measured ice cream sales and the temprature outside on 11 different days, at the end of our modeling **what would be the thing that we would want to predict? - Ice Cream Sales or Temperature?**\n",
    "\n",
    "We would probably want to be measuring temperature with the intention of using that to **forecast** ice cream sales. If we were able to successfully forecast ice cream sales from temperature, this might help us know beforehand how much ice cream to make or how many cones to buy or on which days to open our store, etc. Being able to make predictions accurately has a lot of business implications. This is why making accurate predictions is so valuable (And in large part is why data scientists are paid so well).\n",
    "\n",
    "### Y Variable Intuition\n",
    "\n",
    "We want the thing that we're trying to predict to serve as our **y** variable. This is why it's sometimes called the \"predicted variable.\" We call it the \"dependent\" variable because our prediction for how much ice cream we're going to sell \"depends\" on the temperature outside. \n",
    "\n",
    "### X Variable Intuition\n",
    "\n",
    "All other variables that we use to predict our y variable (we're going to start off just using one) we call our **x** variables. These are called our \"independent\" variables because they don't *depend* on y, they \"explain\" y. Hence they are also referred to as our \"explanatory\" variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Year','Incumbent Party Candidate','Other Candidate','Incumbent Party Vote Share']\n",
    "\n",
    "data = [[1952,\"Stevenson\",\"Eisenhower\",44.6],\n",
    "        [1956,\"Eisenhower\",\"Stevenson\",57.76],\n",
    "        [1960,\"Nixon\",\"Kennedy\",49.91],\n",
    "        [1964,\"Johnson\",\"Goldwater\",61.34],\n",
    "        [1968,\"Humphrey\",\"Nixon\",49.60],\n",
    "        [1972,\"Nixon\",\"McGovern\",61.79],\n",
    "        [1976,\"Ford\",\"Carter\",48.95],\n",
    "        [1980,\"Carter\",\"Reagan\",44.70],\n",
    "        [1984,\"Reagan\",\"Mondale\",59.17],\n",
    "        [1988,\"Bush, Sr.\",\"Dukakis\",53.94],\n",
    "        [1992,\"Bush, Sr.\",\"Clinton\",46.55],\n",
    "        [1996,\"Clinton\",\"Dole\",54.74],\n",
    "        [2000,\"Gore\",\"Bush, Jr.\",50.27],\n",
    "        [2004,\"Bush, Jr.\",\"Kerry\",51.24],\n",
    "        [2008,\"McCain\",\"Obama\",46.32],\n",
    "        [2012,\"Obama\",\"Romney\",52.00]]\n",
    "        \n",
    "df = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x='Year', y='Incumbent Party Vote Share');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Incumbent Party Vote Share'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Incumbent Party Vote Share'\n",
    "df['Prediction'] = df[target].mean()\n",
    "df['Error'] = df['Prediction'] - df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Error'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Absolute Error'] = df['Error'].abs()\n",
    "df['Absolute Error'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Absolute Error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_true=df[target], y_pred=df['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=df[target], y_pred=df['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Year','Average Recent Growth in Personal Incomes']\n",
    "\n",
    "data = [[1952,2.40],\n",
    "        [1956,2.89],\n",
    "        [1960, .85],\n",
    "        [1964,4.21],\n",
    "        [1968,3.02],\n",
    "        [1972,3.62],\n",
    "        [1976,1.08],\n",
    "        [1980,-.39],\n",
    "        [1984,3.86],\n",
    "        [1988,2.27],\n",
    "        [1992, .38],\n",
    "        [1996,1.04],\n",
    "        [2000,2.36],\n",
    "        [2004,1.72],\n",
    "        [2008, .10],\n",
    "        [2012, .95]]\n",
    "        \n",
    "growth = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(growth)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Average Recent Growth in Personal Incomes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x=feature, y=target, kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the scatterplot that these data points seem to follow a somewhat linear relationship. This means that we could probably summarize their relationship well by fitting a line of best fit to these points. Lets do it.\n",
    "\n",
    "\n",
    "## The Equation for a Line\n",
    "\n",
    "As we know a common equation for a line is:\n",
    "\n",
    "\\begin{align}\n",
    "y = mx + b\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the slope of our line and $b$ is the y-intercept. \n",
    "\n",
    "If we want to plot a line through our cloud of points we figure out what these two values should be. Linear Regression seeks to **estimate** the slope and intercept values that describe a line that best fits the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "b = 44\n",
    "df['Prediction'] = m * df[feature] + b\n",
    "df['Error'] = df['Prediction'] - df[target]\n",
    "df['Absolute Error'] = df['Error'].abs()\n",
    "df['Absolute Error'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Absolute Error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_true=df[target], y_pred=df['Prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(x=feature, y=target, kind='scatter')\n",
    "df.plot(x=feature, y='Prediction', kind='line', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(m, b):\n",
    "    df['Prediction'] = m * df[feature] + b\n",
    "    df['Error'] = df['Prediction'] - df[target]\n",
    "    df['Absolute Error'] = df['Error'].abs()\n",
    "    sum_absolute_error = df['Absolute Error'].sum()\n",
    "    \n",
    "    title = f'Sum of absolute errors: {sum_absolute_error}'\n",
    "    ax = df.plot(x=feature, y=target, kind='scatter', title=title, figsize=(7, 7))\n",
    "    df.plot(x=feature, y='Prediction', kind='line', ax=ax)\n",
    "    \n",
    "regression(m=4, b=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Error \n",
    "\n",
    "The residual error is the distance between points in our dataset and our regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(m, b):\n",
    "    df['Prediction'] = m * df[feature] + b\n",
    "    df['Error'] = df['Prediction'] - df[target]\n",
    "    df['Absolute Error'] = df['Error'].abs()\n",
    "    sum_absolute_error = df['Absolute Error'].sum()\n",
    "    \n",
    "    title = f'Sum of absolute errors: {sum_absolute_error}'\n",
    "    ax = df.plot(x=feature, y=target, kind='scatter', title=title, figsize=(7, 7))\n",
    "    df.plot(x=feature, y='Prediction', kind='line', ax=ax)\n",
    "    \n",
    "    for x, y1, y2 in zip(df[feature], df[target], df['Prediction']):\n",
    "        ax.plot((x, x), (y1, y2), color='grey')\n",
    "\n",
    "regression(m=3, b=46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(regression, m=(-10,10,0.5), b=(40,60,0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Square Error'] = df['Error'] **2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(m, b):\n",
    "    df['Prediction'] = m * df[feature] + b\n",
    "    df['Error'] = df['Prediction'] - df[target]\n",
    "    df['Absolute Error'] = df['Error'].abs()\n",
    "    df['Square Error'] = df['Error'] **2\n",
    "    sum_square_error = df['Square Error'].sum()\n",
    "    \n",
    "    title = f'Sum of square errors: {sum_square_error}'\n",
    "    ax = df.plot(x=feature, y=target, kind='scatter', title=title, figsize=(7, 7))\n",
    "    df.plot(x=feature, y='Prediction', kind='line', ax=ax)\n",
    "        \n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    scale = (xmax-xmin)/(ymax-ymin)\n",
    "\n",
    "    for x, y1, y2 in zip(df[feature], df[target], df['Prediction']):\n",
    "        bottom_left = (x, min(y1, y2))\n",
    "        height = abs(y1 - y2)\n",
    "        width = height * scale\n",
    "        ax.add_patch(Rectangle(xy=bottom_left, width=width, height=height, alpha=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(regression, m=(-10,10,0.5), b=(40,60,0.5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 46\n",
    "ms = np.arange(-10,10,0.5)\n",
    "sses = []\n",
    "\n",
    "for m in ms:\n",
    "    predictions = m * df[feature] + b\n",
    "    errors = predictions - df[target]\n",
    "    square_errors = errors ** 2\n",
    "    sse = square_errors.sum()\n",
    "    sses.append(sse)\n",
    "    \n",
    "hypotheses = pd.DataFrame({'Slope': ms})\n",
    "hypotheses['Intercept'] = b\n",
    "hypotheses['Sum of Square Errors'] = sses\n",
    "\n",
    "hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypotheses.plot(x='Slope', y='Sum of Square Errors', \n",
    "                title=f'Intercept={b}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[feature]]\n",
    "y = df[target]\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Prediction'] = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot(x=feature, y=target, kind='scatter', title='sklearn LinearRegression')\n",
    "df.plot(x=feature, y='Prediction', kind='line', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Error'] = df['Prediction'] - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Absolute Error'] = df['Error'].abs()\n",
    "df['Square Error'] = df['Error'] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Square Error'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_true=y, y_pred=model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Squared:  $R^2$\n",
    "\n",
    "One final attribute of linear regressions that we're going to talk about today is a measure of goodness of fit known as $R^2$ or R-squared. $R^2$ is a statistical measure of how close the data are fitted to our regression line. A helpful interpretation for the $R^2$ is the percentage of the dependent variable that is explained by the model.\n",
    "\n",
    "In other words, the $R^2$ is the percentage of y that is explained by the x variables included in the model. For this reason the $R^2$ is also known as the \"coefficient of determination,\" because it explains how much of y is explained (or determined) by our x varaibles. We won't go into the calculation of $R^2$ today, just know that a higher $R^2$ percentage is nearly always better and indicates a model that fits the data more closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.statsmodels.org/dev/examples/notebooks/generated/ols.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Anatomy of Linear Regression\n",
    "\n",
    "- Intercept: The $b$ value in our line equation $y=mx+b$\n",
    "- Slope: The $m$ value in our line equation $y=mx+b$. These two values together define our regression line.\n",
    "\n",
    "![Slope and Intercept](http://www.ryanleeallred.com/wp-content/uploads/2018/08/linear-regression-diagram.png)\n",
    "\n",
    "- $\\hat{y}$ : A prediction\n",
    "- Line of Best Fit (Regression Line)\n",
    "- Predicted (fitted) Values: Points on our regression line\n",
    "- Observed Values: Points from our dataset\n",
    "- Error: The distance between predicted and observed values.\n",
    "\n",
    "![Residual Error](http://www.ryanleeallred.com/wp-content/uploads/2018/08/residual-or-error.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Formal Notation\n",
    "\n",
    "![Simple Linear Regression](http://www.ryanleeallred.com/wp-content/uploads/2018/08/simple-regression-formula.png)\n",
    "\n",
    "We have talked about a line of regression being represented like a regular line $y=mx+b$ but as we get to more complicated versions we're going to need to extend this equation. So lets establish the proper terminology.\n",
    "\n",
    "**X** - Independent Variable, predictor variable, explanatory variable, regressor, covariate\n",
    "\n",
    "**Y** - Response variable, predicted variable, measured vairable, explained variable, outcome variable\n",
    "\n",
    "$\\beta_0$ - \"Beta Naught\" or \"Beta Zero\", the intercept value. This is how much of y would exist if X were zero. This is sometimes represented by the letter \"a\" but I hate that. So it's \"Beta 0\" during my lecture.\n",
    "\n",
    "$\\beta_1$ - \"Beta One\" The primary coefficient of interest. This values is the slope of the line that is estimated by \"minimizing the sum of the squared errors/residuals\" - We'll get to that. \n",
    "\n",
    "$\\epsilon$ - \"Epsilon\" The \"error term\", random noise, things outside of our model that affect y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Does it do it?\n",
    "\n",
    "## Minimizing the Sum of the Squared Error\n",
    "\n",
    "The most common method of estimating our $\\beta$ parameters  is what's known as \"Ordinary Least Squares\" (OLS). (There are different methods of arriving at a line of best fit). OLS estimates the parameters that minimize the squared distance between each point in our dataset and our line of best fit. \n",
    "\n",
    "\\begin{align}\n",
    "SSE = \\sum(y_i - \\hat{y})^2\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra!\n",
    "\n",
    "The same result that is found by minimizing the sum of the squared errors can be also found through a linear algebra process known as the \"Least Squares Solution:\"\n",
    "\n",
    "![OLS Regression](http://www.ryanleeallred.com/wp-content/uploads/2018/08/OLS-linear-algebra.png)\n",
    "\n",
    "Before we can work with this equation in its linear algebra form we have to understand how to set up the matrices that are involved in this equation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The $\\beta$ vector\n",
    "\n",
    "The $\\beta$ vector represents all the parameters that we are trying to estimate, our $y$ vector and $X$ matrix values are full of data from our dataset. The $\\beta$ vector holds the variables that we are solving for: $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "Now that we have all of the necessary parts we can set them up in the following equation:\n",
    "\n",
    "\\begin{align}\n",
    "y = X \\beta + \\epsilon\n",
    "\\end{align}\n",
    "\n",
    "Since our $\\epsilon$ value represents **random** error we can assume that it will equal zero on average.\n",
    "\n",
    "\\begin{align}\n",
    "y = X \\beta\n",
    "\\end{align}\n",
    "\n",
    "The objective now is to isolate the $\\beta$ matrix. We can do this by pre-multiplying both sides by \"X transpose\" $X^{T}$.\n",
    "\n",
    "\\begin{align}\n",
    "X^{T}y =  X^{T}X \\beta\n",
    "\\end{align}\n",
    "\n",
    "Since anything times its transpose will result in a square matrix, if that matrix is then an invertible matrix, then we should be able to multiply both sides by its inverse to remove it from the right hand side. (We'll talk tomorrow about situations that could lead to $X^{T}X$ not being invertible.)\n",
    "\n",
    "\\begin{align}\n",
    "(X^{T}X)^{-1}X^{T}y =  (X^{T}X)^{-1}X^{T}X \\beta\n",
    "\\end{align}\n",
    "\n",
    "Since any matrix multiplied by its inverse results in the identity matrix, and anything multiplied by the identity matrix is itself, we are left with only $\\beta$ on the right hand side:\n",
    "\n",
    "\\begin{align}\n",
    "(X^{T}X)^{-1}X^{T}y = \\hat{\\beta}\n",
    "\\end{align}\n",
    "\n",
    "We will now call it \"beta hat\" $\\hat{\\beta}$ because it now represents our estimated values for $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "### Lets calculate our $\\beta$ coefficients with numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sm.add_constant(df[feature]).values\n",
    "print('X')\n",
    "print(X)\n",
    "\n",
    "y = df[target].values[:, np.newaxis]\n",
    "print('y')\n",
    "print(y)\n",
    "\n",
    "X_transpose = X.T\n",
    "print('X Transpose')\n",
    "print(X_transpose)\n",
    "\n",
    "X_transpose_X = X_transpose @ X\n",
    "print('X Transpose X')\n",
    "print(X_transpose_X)\n",
    "\n",
    "X_transpose_X_inverse = np.linalg.inv(X_transpose_X)\n",
    "print('X Transpose X Inverse')\n",
    "print(X_transpose_X_inverse)\n",
    "\n",
    "X_transpose_y = X_transpose @ y\n",
    "print('X Transpose y')\n",
    "print(X_transpose_y)\n",
    "\n",
    "beta_hat = X_transpose_X_inverse @ X_transpose_y\n",
    "print('Beta Hat')\n",
    "print(beta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Regression\n",
    "\n",
    "Simple or bivariate linear regression involves a single $x$ variable and a single $y$ variable. However, we can have many $x$ variables. A linear regression model that involves multiple x variables is known as **Multiple** Regression - NOT MULTIVARIATE!\n",
    "\n",
    "![Multiple Regression](http://www.ryanleeallred.com/wp-content/uploads/2018/08/multiple-regression-model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Square Error', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fatalities denotes the cumulative number of American military\n",
    "fatalities per millions of US population the in Korea, Vietnam,\n",
    "Iraq and Afghanistan wars during the presidential terms\n",
    "preceding the 1952, 1964, 1968, 1976 and 2004, 2008 and\n",
    "2012 elections.\n",
    "\n",
    "http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf\n",
    "\"\"\"\n",
    "\n",
    "columns = ['Year','US Military Fatalities per Million']\n",
    "\n",
    "data = [[1952,190],\n",
    "        [1956,  0],\n",
    "        [1960,  0],\n",
    "        [1964,  1],\n",
    "        [1968,146],\n",
    "        [1972,  0],\n",
    "        [1976,  2],\n",
    "        [1980,  0],\n",
    "        [1984,  0],\n",
    "        [1988,  0],\n",
    "        [1992,  0],\n",
    "        [1996,  0],\n",
    "        [2000,  0],\n",
    "        [2004,  4],\n",
    "        [2008, 14],\n",
    "        [2012,  5]]\n",
    "        \n",
    "deaths = pd.DataFrame(data=data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Average Recent Growth in Personal Incomes', \n",
    "            'US Military Fatalities per Million']\n",
    "\n",
    "target = 'Incumbent Party Vote Share'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "print('Intercept:', model.intercept_)\n",
    "pd.Series(model.coef_, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y, model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.query('Year < 2008')\n",
    "test  = df.query('Year >= 2008')\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_test  = test[features]\n",
    "y_test  = test[target]\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More about the \"Bread & Peace\" model\n",
    "- https://fivethirtyeight.com/features/what-do-economic-models-really-tell-us-about-elections/\n",
    "- https://statmodeling.stat.columbia.edu/2007/12/15/bread_and_peace/\n",
    "- https://avehtari.github.io/RAOS-Examples/ElectionsEconomy/hibbs.html\n",
    "- https://douglas-hibbs.com/\n",
    "- http://www.douglas-hibbs.com/HibbsArticles/HIBBS-PRESVOTE-SLIDES-MELBOURNE-Part1-2014-02-26.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality in Linear Regression!\n",
    "\n",
    "Muliple Regression is simply an extension of the bivariate case. The reason why we see the bivariate case demonstrated so often is simply because it's easier to graph and all of the intuition from the bivariate case is the same as we keep on adding explanatory variables.\n",
    "\n",
    "As we increase the number of $x$ values in our model we are simply fitting a n-1-dimensional plane to an n-dimensional cloud of points within an n-dimensional hypercube. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Coefficients\n",
    "\n",
    "One of Linear Regression's strengths is that the parameters of the model (coefficients) are readily interpretable and useful. Not only do they describe the relationship between x and y but they put a number on just how much x is associated with y. We should be careful to not speak about this relationshiop in terms of causality because these coefficients are in fact correlative measures. We would need a host of additional techniques in order to estimate a causal effect using linear regression (econometrics).\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\beta} = \\frac{Cov(x,y)}{Var(y)}\n",
    "\\end{align}\n",
    "\n",
    "Going back to the two equations for the two models that we have estimated so far, lets replace their beta values with their actual values to see if we can make sense of how to interpret these beta coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate Model\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1temperature + \\epsilon$\n",
    "\n",
    "$sales_i = -596.2 + 24.69temperature + \\epsilon$\n",
    "\n",
    "What might $\\beta_0$ in this model represent? It represents the level of sales that we would have if temperature were 0. Since this is negative one way of interpreting it is that it's so cold outside that you would have to pay people to eat ice cream. A more appropriate interpretation is probably that the ice cream store owner should close his store down long before the temperature reaches 0 degrees farenheit (-17.7 celsius). The owner can compare his predicted sales with his costs of doing business to know how warm the weather has to get before he should open his store.\n",
    "\n",
    "What might the $beta_1$ in this model reprsent? it represents the increase in sales for each degree of temperature increase. For every degree that the temperature goes up outside he has $25 more in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Regression Model\n",
    "\n",
    "$y_i = \\beta_0 + \\beta_1age_i + \\beta_2weight_i + \\epsilon$\n",
    "\n",
    "$BloodPressure_i = 30.99+ .86age_i + .33weight_i + \\epsilon$\n",
    "\n",
    "The interpretation of coefficients in this example are similar. The intercept value repesents the blood pressure a person would have if they were 0 years old and weighed 0 pounds. This not a super useful interpretation. If we look at our data it is unlikely that we have any measurements like these in the dataset. This means that our interpretation of our intercept likely comes from extrapolating the regression line (plane). Coefficients having straightforward interpretations is a strength of linear regression if we're careful about extrapolation and only interpreting our data within the context that it was gathered.\n",
    "\n",
    "The interpretation of our other coefficients can be a useful indicator for how much a person similar to those in our dataset's blood pressure will go up on average with each additional year of age and pound of weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model Validation\n",
    "\n",
    "One of the downsides of relying on $R^2$ too much is that although it tells you when you're fitting the data well, it doesn't tell you when you're *overfitting* the data. The best way to tell if you're overfitting the data is to get some data that your model hasn't seen yet, and evaluate how your predictions do. This is essentially what \"model validation\" is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is Linear Regression so Important?\n",
    "\n",
    "## Popularity \n",
    "\n",
    "Linear Regression is an extremely popular technique that every data scientist **needs** to understand. It's not the most advanced technique and there are supervised learning techniques that will obtain a higher accuracy, but where it lacks in accuracy it makes up for it in interpretability and simplicity.\n",
    "\n",
    "## Interpretability\n",
    "\n",
    "Few other models possess coefficients that are so directly linked to their variables with a such a clear interpretation. Tomorrow we're going to learn about ways to make them even easier to interpret.\n",
    "\n",
    "## Simplicity\n",
    "\n",
    "A linear regression model can be communicated just by writing out its equation. It's kind of incredible that such high dimensional relationships can be described from just a linear combination of variables and coefficients. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
