{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science_\n",
    "\n",
    "# Polynomial & Log-Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1o9Qe8ilN19"
   },
   "source": [
    "## \"Linear\" Regression?\n",
    "\n",
    "Which of the following is a linear regression model?\n",
    "\n",
    "![Functional Form Misspecification](http://www.ryanleeallred.com/wp-content/uploads/2018/08/functional-form-misspecification.jpg)\n",
    "\n",
    "**All** of these functional forms can be fit using Linear Regression. The \"Linear\" in linear regression refers to the linear form of the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4a5raiR9doDx"
   },
   "source": [
    "### Linear Combinations\n",
    "\n",
    "Remember when we rewrote vectors as a **linear combination** of scalars and unit vectors?\n",
    "\n",
    "\\begin{align}\n",
    "v = \\begin{bmatrix}2 \\\\ 3\\end{bmatrix} = 2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 3 \\begin{bmatrix} 0 \\\\ 1\\end{bmatrix} = 2\\hat{i} + 3\\hat{j} \n",
    "\\end{align}\n",
    "\n",
    "The syntax where we have a scalar (think coefficient) multiplying some vector (unit vector in this case) and all of them being added together is what makes this a linear combination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gu-5ZGkFdpCS"
   },
   "source": [
    "### Linear Equations\n",
    "\n",
    "A \"Linear Equation\" is any equation that takes the following form: \n",
    "\n",
    "\\begin{align}\n",
    "a_1x_1 + \\ldots + a_nx_n + b = 0\n",
    "\\end{align}\n",
    "\n",
    "Does this look familiar? A linear equation is one where we have $x_1, \\ldots, x_n$ unknowns and $b, a_1, \\ldots, a_n$ coefficients which are considered parameters of the equation. \"The solutions of such an equation are the values that, when substituted to the unknowns, make the equality true.\"\n",
    "\n",
    "[Linear Equation Wikipedia](https://en.wikipedia.org/wiki/Linear_equation)\n",
    "\n",
    "Linear Regression is **linear** not because it can only plot straight lines and fit straight-line patterns in data, but because the form of the equation used to represent our regression is in the form of a **Linear Equation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUMyYQSM_w-9"
   },
   "source": [
    "### _So how do we fit curved data?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example #1: Moore's Law dataset\n",
    "\n",
    "#### Background\n",
    "- https://en.wikipedia.org/wiki/Moore%27s_law\n",
    "- https://en.wikipedia.org/wiki/Transistor_count\n",
    "\n",
    "#### Scrape HTML tables with Pandas!\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html\n",
    "- https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58\n",
    "\n",
    "#### More web scraping options\n",
    "- https://automatetheboringstuff.com/chapter11/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3K1aHn-3BdD"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html('https://en.wikipedia.org/wiki/Transistor_count', header=0)\n",
    "print([table.shape for table in tables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moore = tables[0]\n",
    "moore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Date of introduction'\n",
    "target  = 'Transistor count'\n",
    "\n",
    "moore = moore.dropna(subset=[feature, target]).copy()\n",
    "\n",
    "for column in [feature, target]:\n",
    "    moore[column] = (moore[column]\n",
    "                     .str.split('[').str[0]  # Remove citations\n",
    "                     .str.replace(r'\\D','')  # Remove non-digit characters\n",
    "                     .astype(int))\n",
    "    \n",
    "moore = moore.sort_values(by=feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(moore[target]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moore.plot(x=feature, y=target, kind='scatter', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moore.plot(x=feature, y=target, kind='scatter', alpha=0.5, logy=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5)\n",
    "\n",
    "X = moore[[feature]]\n",
    "y = moore[target]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "ax.plot(X, model.predict(X))\n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5, logy=True)\n",
    "\n",
    "X = moore[[feature]]\n",
    "y = np.log(moore[target]) # Apply natural log function to the target\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "y_pred = np.exp(model.predict(X)) # Apply exponential function (inverse of natural log) to the predictions\n",
    "ax.plot(X, y_pred) \n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5)\n",
    "ax.plot(X, y_pred)\n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make polynomial feature\n",
    "moore['Date of introduction ** 2'] = moore['Date of introduction'] ** 2\n",
    "\n",
    "features = ['Date of introduction', 'Date of introduction ** 2']\n",
    "X = moore[features]\n",
    "y = moore[target]\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "ax = moore.plot(x='Date of introduction', y=target, kind='scatter', alpha=0.5)\n",
    "ax.plot(X['Date of introduction'], model.predict(X))\n",
    "print('R^2', model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize for higher degree polynomials, and print equation\n",
    "\n",
    "def polynomial_regression(degrees=2):\n",
    "    \n",
    "    # Make polynomial features\n",
    "    feature = 'Date of introduction'\n",
    "    polynomial_features = []\n",
    "    for degree in range(2, degrees+1):\n",
    "        name = f'{feature} ** {degree}'\n",
    "        moore[name] = moore[feature] ** degree\n",
    "        polynomial_features.append(name)\n",
    "    \n",
    "    features = [feature] + polynomial_features\n",
    "    target  = 'Transistor count'\n",
    "    X = moore[features]\n",
    "    y = moore[target]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = moore.plot(x=feature, y=target, kind='scatter', alpha=0.5)\n",
    "    ax.plot(moore[feature], model.predict(X))\n",
    "    betas = [model.intercept_] + model.coef_.tolist()\n",
    "    equation = ' + '.join(f'{beta}x**{i}' for i, beta in enumerate(betas))\n",
    "    print(equation)\n",
    "    print('R^2', model.score(X, y))\n",
    "    \n",
    "polynomial_regression(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(polynomial_regression, degrees=(1,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cs8PskIdy9t"
   },
   "source": [
    "# Polynomial Regression, explained\n",
    "\n",
    "Just as multiple regression was an extension of the bivariate case, Polynomial Regression is an extention of multiple regression and can be used to fit data to any (curved) shape. This is one of the reasons why data exploration is so important. You won't know that you need to fit a polynomial function to a feature unless you have examined its distribution.\n",
    "\n",
    "[Why is polynomial regression considered a special case of multiple linear regression?](https://stats.stackexchange.com/questions/92065/why-is-polynomial-regression-considered-a-special-case-of-multiple-linear-regres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptmg8FRty5Pu"
   },
   "source": [
    "# Example #1: King County Housing Data\n",
    "\n",
    "[from Kaggle](https://www.kaggle.com/harlfoxem/housesalesprediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1840,
     "status": "ok",
     "timestamp": 1550599140263,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "9dEQTcwHy8mY",
    "outputId": "7928f778-38e2-4a74-f2df-53e32cc4764e"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv')\n",
    "print(df.shape)\n",
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCwbKGIOzqbO"
   },
   "source": [
    "## Find a \"curved\" feature in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "for feature in numeric_columns.drop(target):\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H6X-TDuL2PaG"
   },
   "source": [
    "## Make or \"engineer\" a new grade_squared feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16806,
     "status": "ok",
     "timestamp": 1550599156323,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "lKiJSmUn2Ohr",
    "outputId": "c8a41385-5613-403c-e193-cf91d31febf3"
   },
   "outputs": [],
   "source": [
    "df['grade_squared'] = df['grade']**2\n",
    "for feature in ['grade', 'grade_squared']:\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTdqhfQY1hLb"
   },
   "source": [
    "## Test the fit of a polynomial regression to that feature\n",
    "\n",
    "First we'll fit a regular bivariate regression line and calculate its $R^2$ to get a baseline. Since we want to know if this generated feature is improving our model or not we'll first run our code without it so that we have something to compare to. \n",
    "\n",
    "$price_i = \\beta_0 + \\beta_1grade_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32552,
     "status": "ok",
     "timestamp": 1550599172729,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "l5eFe55l2JHT",
    "outputId": "578adb74-904b-49c7-ddd2-d7c1600f637f"
   },
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "target = 'price'\n",
    "features = ['grade']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "def run_linear_model(X, y):\n",
    "    # Split into test and train data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Fit model using train data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions using test features\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compare predictions to test target\n",
    "    rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print('Root Mean Squared Error', rmse)\n",
    "    print('R^2 Score', r2)\n",
    "    print('Intercept', model.intercept_)\n",
    "    coefficients = pd.Series(model.coef_, X_train.columns)\n",
    "    print(coefficients.to_string())\n",
    "    \n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4dAZb0U4-vE"
   },
   "source": [
    "## Lets try fitting grade_squared as a bivariate model\n",
    "\n",
    "$price_i = \\beta_0 + \\beta_1grade^{2}_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32093,
     "status": "ok",
     "timestamp": 1550599172730,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "Nfe7mTsm4-Bv",
    "outputId": "2682e202-bfda-4c81-bded-3dff017e592a"
   },
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "target = 'price'\n",
    "features = ['grade_squared']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lSw_KYF5wSB"
   },
   "source": [
    "## Multiple Regression using both grade and grade_squred\n",
    "\n",
    "$price_i = \\beta_0 + \\beta_1 grade_i + \\beta_2grade^{2}_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31694,
     "status": "ok",
     "timestamp": 1550599172732,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "avqSfsJr2h2I",
    "outputId": "0d932f23-6c1b-417f-e7ee-8372b4d2561e"
   },
   "outputs": [],
   "source": [
    "# Separate dependent and independent variables\n",
    "target = 'price'\n",
    "features = ['grade', 'grade_squared']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "loZ378cZ7mIC"
   },
   "source": [
    "# How to find non-linear features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fM6fKuubBy8b"
   },
   "source": [
    "## 1) Domain Knowledge (think about your variables and hypothesize)\n",
    "\n",
    "This is why having domain knowledge about the problem that you're trying to solve is something that's so important. In the context of home prices, variables that have a curved structure often are that way due to some form of diminishing returns increases in certain amenities. Lets think about the following variables:\n",
    "\n",
    "- Lot Size:\n",
    "\n",
    "The more land you're willing to buy all at once the cheaper it will be on a per-acre basis (Saving money when you buy in bulk). This trend carries through to small to medium sized lots but but with a more shallow curve.\n",
    "\n",
    "![Price Per Acre](https://placercountyhomesandland.typepad.com/photos/uncategorized/price_per_acre_graph.png)\n",
    "\n",
    "- Square Footage:\n",
    "\n",
    "Square footage of a home sees a similar pattern. The value an additional 100 square feet in small homes (imagine the difference between say a 800 sq foot home and a 900 sq foot home) makes a big difference to buyers, where as an additional 100 square feet in a mansion probably isn't valued quite as highly. \n",
    "\n",
    "- Age:\n",
    "\n",
    "Just like how the prices of new cars drop steeply in the first few years, the value of homes due to age drop quickly in the first few years after its built and then less quickly as time goes on. This is not a linear pattern and needs to be fitted by a polynomial model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f5cBlL1xBzFw"
   },
   "source": [
    "## 2) Visual Inspection\n",
    "\n",
    "We already talked about how generating scatterplots or other graphs is vital in our data exploration stage and can lead us to identify possible candidates for polynomials. Here I just wanted to share one more tip that can help you analyze scatterplots when you have a lot of data.\n",
    "\n",
    "If you have so much data that it's hard to tell what's going on in your scatterplot, then sample your data and regenerate them to get a better idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39338,
     "status": "ok",
     "timestamp": 1550599181362,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "udIeJAY-B94U",
    "outputId": "9cec3697-7aa8-459f-8e39-7e8c2e2cc513"
   },
   "outputs": [],
   "source": [
    "# Sample our dataframe to take 1/20th the values\n",
    "sampled = df.sample(frac=0.05, replace=True, random_state=42)\n",
    "\n",
    "target = 'price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "for feature in numeric_columns.drop(target):\n",
    "    sns.scatterplot(x=feature, y=target, data=sampled, alpha=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDyTStZgBzIU"
   },
   "source": [
    "## 3) Inspect the distribution of residuals\n",
    "\n",
    "![Poor Fit Residuals](http://www.thejavageek.com/wp-content/uploads/2018/02/linear-regression-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRF9aISzGRyr"
   },
   "source": [
    "## An Aside: The \"Hedonic Housing Model\"\n",
    "\n",
    "Using Linear Regression to model home prices is a very common use of predictive linear regression modeling. It's so common fact that it has its own name: The Hedonic Housing Model. In the Hedonic Housing model it is well understood and reiterated that certain features tend to be curved in nature and these polynomial features (like the ones mentioned above) have all been well explored in real estate prediction circles. This is another of how domain knowledge can give you an edge. The best way to gain domain knowledge is to dive in and try and solve one particular kind of problem, and pick up little tips and tidbits as time goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1xiwKaUP0Y_"
   },
   "source": [
    "# Log-Linear Regression\n",
    "\n",
    "In a log-linear regression model, we take the natural log of all of our y variable and use that as our y vector instead of the raw y values. Why would we do that?\n",
    "\n",
    "<https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/60861>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kVJde3zZrms"
   },
   "source": [
    "## 1) To reduce skew in y\n",
    "\n",
    "Where we have variables with lots of relatively low values and few high values (like with home prices) we would expect to see our data more clustered on the left-hand side with a long tail extending to the right up into the expensive homes. The fact of the matter is that we will be able to make better predictions if we can normalize our data to some degree and one way to do this is by taking the natural log of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdS22S_OQ-vr"
   },
   "outputs": [],
   "source": [
    "df['ln_price'] = np.log(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37701,
     "status": "ok",
     "timestamp": 1550599181593,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "YqmZNtUOTZoN",
    "outputId": "6125f6ed-2df1-404e-d977-32135f41bf00"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['price']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37533,
     "status": "ok",
     "timestamp": 1550599181805,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "p7CDc9_ETeUf",
    "outputId": "cd9f3f3f-9936-4e2d-c3fb-2482ca04ddc9"
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['ln_price']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXrEJWb8akIL"
   },
   "source": [
    "## New distribution of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52845,
     "status": "ok",
     "timestamp": 1550599197819,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "whrf7MbDYisJ",
    "outputId": "beffe32c-c216-46fd-97f9-cd9619ad3955"
   },
   "outputs": [],
   "source": [
    "target = 'ln_price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "for feature in numeric_columns.drop(target):\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sd4mICl9ZrvR"
   },
   "source": [
    "## 2) Make coefficients easier to interpret\n",
    "\n",
    "Transforming our price values in this way won't change our model's ability to generate predictions, but what it **will** do is change the interpretation of all of our x-coefficients. This will change our x-coefficients from have an elasticity type interpretation (a raw dollar amount change if there is a 1 unit increase in x) to having a percentage-terms interpretation. Lets demonstrate and talk about this further. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-Yt7EcZZr33"
   },
   "source": [
    "## 3) Make our errors easier to interpret\n",
    "\n",
    "Errors that have been calculated on variables that are in log form can also be interpreted roughly as percentage error. We've been using percentages all our lives and they have immediate meaning to us. This is why I prefer log-linear regression models when possible.\n",
    "\n",
    "<https://people.duke.edu/~rnau/411log.htm>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1ecoaG9K-FB"
   },
   "source": [
    "Feature Engineering is a big topic in machine learning. We won't be able to cover every aspect of it today, but hopefully we can give you a strong idea of what it is and how to go about it. \n",
    "\n",
    "[Understanding Feature Engineering Part 1](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)\n",
    "\n",
    "Feature engineering is key to success in predictive modeling. It is the process by which we take existing features and combine them or alter them in ways that will expose additional signal to our model. Feature engineering is all about making the most of the data that we already had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1550599291855,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "gLC6Zz1pQaef",
    "outputId": "11b76865-0d19-4617-8047-6c114f809f0a"
   },
   "outputs": [],
   "source": [
    "# Log-Linear Regression\n",
    "# Separate dependent and independent variables\n",
    "target = 'ln_price'\n",
    "features = ['grade']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwlCapGPUwFe"
   },
   "source": [
    "This means that a one unit increase in the grade of a home increases its sale price by 31%. Often it is much easier to interpret coefficients in this manner than in the regular way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 50803,
     "status": "ok",
     "timestamp": 1550599197821,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "ZVNHMgpuXo5z",
    "outputId": "460e0b13-54ae-4ae9-82d3-09078d3007ee"
   },
   "outputs": [],
   "source": [
    "## Log-Linear Regression\n",
    "# Separate dependent and independent variables\n",
    "target = 'ln_price'\n",
    "features = ['grade', 'grade_squared']\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCdxF6o1Rowb"
   },
   "source": [
    "Our RMSE is really \"small\" now because it now represents error in percentage terms. We're on average about 37% off in our predictions of house prices.\n",
    "\n",
    "Our coefficients can also be understood in percentage terms which makes the coefficients on our regression much more digestable at a glance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O5TODtjOJZzr"
   },
   "source": [
    "## A note about $R^2$\n",
    "\n",
    "$R^2$ If we add any feature to our model (even nearly meaningless ones) our $R^2$ will improve. For this reason raw $R^2$ is not the ultimate measure of goodness of fit. It is informative but completely secondary to our Root-Mean-Squared-Error (in predictive modeling). While a higher $R^2$ is generally better, this isn't the thing that we're trying to optimize. We care more about minimizing RMSE than maximizing $R^2$\n",
    "\n",
    "### \"Kitchen Sink\" models\n",
    "\n",
    "You may hear the term \"kitchen sink\" regression model used. This refers to a regression model that throws every available explanatory variable into the model in an attempt to improve it without much thought to whether those variables should really be considered as affecting y. Kitchen Sink models will have a higher $R^2$ than others but will have higher standard errors (estimates about particular coefficients may be less precise). \n",
    "\n",
    "Therefore, you tend to see \"Kitchen Sink\" models when the only priority is predictive accuracy and not interpretability. \n",
    "\n",
    "### Alternative measures of Goodness-of-fit\n",
    "\n",
    "Efforts have been made to improve upon $R^2$ due to these limitations. A metric called \"Adjusted $R^2$\" seeks to account for the number of explanatory variables included in a model and adjust $R^2$ accordingly. This is something that you can look up if you're curious. I just wanted to make you aware of it more than anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRh1C4qJXWY3"
   },
   "source": [
    "## A note about dirty linear regression data\n",
    "\n",
    "### Linear Regression models can only process numeric values\n",
    "\n",
    "### Your data must be free of NaN values before being passed to the algorithm \n",
    "\n",
    "(some data cleaning will be required in today's assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "36wffNrwZJ6V"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4B98U_YoMBTY"
   },
   "source": [
    "### Polynomial Regression requires feature engineering\n",
    "\n",
    "You've already seen an example of feature engineering today when we created the $grade^2$ variable. We took an existing feature and used it to generate a new feature that exposed the data to the model in a slightly different way. \n",
    "\n",
    "\"Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\" - [Jason Brownlee](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "\n",
    "### What features could we engineer with the King County dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1547533732470,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "WKs8ww9yj3e3",
    "outputId": "9021a38b-cf80-4870-d9d0-a2d2e29ca2cc"
   },
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pk15gYRxMw9_"
   },
   "source": [
    "- **[date]** The date is in a format that is not super useful to us. If were to extract the year we could then take the difference between year and yr_built to find the age of the home. We could also include the squared term of the home age since we know (from our hedonic housing model domain knowledge) that home age typically is not linear. \n",
    "- **[bedrooms]** & **[bathrooms]** We maybe use a combined measure of bedrooms and bathrooms, or find an average room square footage by taking total number of rooms and dividing by the square footage. \n",
    "- **[sqft_living]** **[sqft_lot]** The difference between lot square footage and home square footage ought to also give us a rough measure of the size of the yard. Rough measures are fine as long as the engineered features expose some new shred of meaning to our model.\n",
    "- **[floors]** We could calculate an average number of square feet per floor\n",
    "- **[lat]** **[long]** There are all kinds of things that we could do with the latitude and longitude especially if we use some kind of outside API or external dataset to bring in new features associated with the location of the homes. This would take a lot of work but these could potentially be very powerful features.\n",
    "\n",
    "### Kaggle is one of the best places to get feature engineering ideas\n",
    "\n",
    "Look at Kernels: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/kernels\n",
    "\n",
    "Here's just one example: https://www.kaggle.com/thevachar/house-price-regression-and-feature-engineering\n",
    "\n",
    "Kaggle has definitions of the columns too: https://www.kaggle.com/harlfoxem/housesalesprediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with baseline, before feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1550602599055,
     "user": {
      "displayName": "Ryan Allred",
      "photoUrl": "",
      "userId": "04031804316926795705"
     },
     "user_tz": 420
    },
    "id": "ESFfcliEgt9y",
    "outputId": "31ef7d05-9969-442b-b294-b6a003fa7a25"
   },
   "outputs": [],
   "source": [
    "df['ln_price'] = np.log(df['price'])\n",
    "target = 'ln_price'\n",
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "features = numeric_columns.drop([target, 'price', 'id'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGHxukhjHsWu"
   },
   "outputs": [],
   "source": [
    "# Non-Feature Engineered Baseline Model\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineer a new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD0a0JyUInGu"
   },
   "outputs": [],
   "source": [
    "df['age'] = 2015 - df['yr_built']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = df.select_dtypes(include='number').columns\n",
    "features = numeric_columns.drop([target, 'price', 'id'])\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "run_linear_model(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you engineer more features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Nww7246e4pn"
   },
   "source": [
    "## More topics\n",
    "\n",
    "### Interaction Terms\n",
    "\n",
    "An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable.\n",
    "\n",
    "Lets look at an example where we were trying to estimate the level of satisfaction that a person would have when eating some kind of food with a condiment (sauce) on it.\n",
    "\n",
    "$satisfaction_i = \\beta_0 + \\beta_1 food_i + \\beta_2condiment_i + \\epsilon$\n",
    "\n",
    "Imagine that we have two foods: Ice Cream and Hot Dogs, and we also have to condiments: hot fudge and mustard. \n",
    "\n",
    "$\\beta_1$ in this example is trying to capture the effect of on satisfaction between eating hot dogs vs eating ice cream, and $\\beta_2$ is trying to capture the effect of putting hot fudge (chocolate sauce) vs mustard on your food. \n",
    "\n",
    "$\\beta_2$ is a little more problematic in this scenario. If someone were to come up to you and ask if you preferred hot fudge or mustard on your food, how would you answer?\n",
    "\n",
    "You would probably say something like **\"IT DEPENDS ON WHAT THE FOOD IS.\"** This means that the effect of our x variables on y (satisfaction) depends on the combination of food and condiment. I don't know about you guys, but I wouldn't be as satisfied if I had hot fudge on my hot dog or mustard on my ice cream. \n",
    "\n",
    "An interaction terms is something that we add to our regression to account for these \"It Depends\" moments between two x variables. We do this by multiplying the two of them together or *interacting* them with each other to capture the implications of the different combinations taking place.\n",
    "\n",
    "$satisfaction_i = \\beta_0 + \\beta_1 food_i + \\beta_2condiment_i + \\beta_3(food\\times condiment_i) + \\epsilon$\n",
    "\n",
    "<http://statisticsbyjim.com/regression/interaction-effects/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ZH-5O0lLbJ7"
   },
   "source": [
    "### Removing Outliers\n",
    "\n",
    "To remove outliers via the 1.5*Interquartile-Range method. The first step is to calculate the IQR for each variable.\n",
    "\n",
    "The IQR is the difference between the 25th and 75th percentiles of the feature.\n",
    "\n",
    "Find the IQR and multiply it by 1.5\n",
    "\n",
    "Then add the 1.5*IQR to the 3rd quartile (75th percentile). Anything above that range is an outlier.\n",
    "Subtract 1.5*IQR from the 1st quartile (25th percentile). Anything below that value is also an outlier.\n",
    "\n",
    "You want to minimize outliers in your dataset, so remove them by dropping observations that contain outliers in key features.\n",
    "\n",
    "Typically you will wan to remove outliers before doing anything else with your dataset. We haven't focused on this strongly yet in the class, but coefficients get strongly biased by outliers so if you want to really have accurate predictions, remove outliers before you begin your feature engineering and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQ08zZkxW0hY"
   },
   "source": [
    "# Major Takeaways\n",
    "\n",
    "- Polynomial Regression\n",
    "  - Linear Regression can fit curved lines.\n",
    "  - Including squared and cubed terms can improve fit and accuracy.\n",
    "\n",
    "- Log-linear Regression\n",
    "  - ln(y) helps normalize data with a skewed y variable.\n",
    "  - ln(y) changes interpretation of coefficients and errors to be percentages.\n",
    "\n",
    "- Feature Engineering\n",
    "  - Generating Features improves model accuracy if done well.\n",
    "  - This is where creativity and domain knowledge really pay off.\n",
    "  - When you think that certain combinations of x variables might affect y differently than how they do separately. Include an interaction term."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Polynomial/Log-linear Regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
