{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "YT0Z0S653S0O",
    "outputId": "ed893f11-6743-4f4f-fff2-a44f2300d791"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lnGXseZQoB2z"
   },
   "source": [
    "# Regression Diagnostics\n",
    "\n",
    "Whenever you undertake regression analysis of any kind you should run diagnostic tests that check the shape of your data and the fit of your model to that data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BrvzSNdipyec"
   },
   "source": [
    "## Not common in predictive modeling\n",
    "\n",
    "You won't see many Kaggle competitions running these tests because they aren't as important for predictive modeling. There are less important (and sometimes completely ignored) in predictive modeling because the end-all be-all of predictive modeling is how accurate your model's predctions are on an \"out of sample\" dataset. This is why we split our dataset into two random halves and and then fit our model parameters using one half, and test the accuracy of our model's predictions using the other half. (It doesn't have to be 50-50 necessarily, but just an example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eTeKii_ZuG99"
   },
   "source": [
    "## Necessary for inferential regression modeling\n",
    "\n",
    "However, if you ever need to run regression analysis for the purposes of inferentital modeling --because you intend to interpret and be informed by variable coefficients --these tests are of utmost importance. Each of these tests exists to test a certain assumption that we're making about the shape of our data or our model's fit to it. If one or multipile of these assumptions are violated, then doubt is cast on the reliability of our variable coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4VLj0NQuwbw"
   },
   "source": [
    "# Estimating Parameters\n",
    "\n",
    "You'll remember that OLS and Gradient-Descent based methods of linear regression modeling both seek to **estimate** parameters that \"minimize the sum of the squared error.\" Because we have been more focused on predictive modeling we haven't talked as much about what it means for a parameter to be an \"estimate.\"\n",
    "\n",
    "An estimated regression coefficient represents the **mean** change in our response variable (y) given a one unit change in our response given a one unit change in the predictor. But because it is an estimate, there is a certain confidence interval around our prediction of our coefficient. The confidence interval is vital to our interpretation of regression coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFGAFOg22YlD"
   },
   "source": [
    "## A Parameter Estimation Example\n",
    "\n",
    "Suppose I was fitting a regression model and calculated its coefficients and substituted them into the equation:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = .42+ 2.05x\n",
    "\\end{align}\n",
    "\n",
    "We've well established in past lectures that $\\hat{\\beta}_1$ reprents the slope of our regression line, but we haven't talked about how this is just an **estimate** for the slope of our regression line, and as an estimate has an associated confidence interval.\n",
    "\n",
    "Lets say that we calculated the 95% confidence interval for $\\hat{\\beta}_1$ and it came out to be $(1.9 ,  2.2)$. This means that we can only be 95% confident that the average effect of x on y is within this range. Up to this point we have just taken the reported coefficient as gospel, but a lot of conditions need to be satisfied in order for us to trust regression coefficients. We'll talk about a few of them today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "M3rUIaxZ2paP",
    "outputId": "4f1ab705-faf6-496b-8720-bf399e1e9bdc"
   },
   "outputs": [],
   "source": [
    "# We can create scatterplots that show the confidence interval!\n",
    "\n",
    "heights = np.array([50,52,53,54,58,60,62,64,66,67, 68,70,72,74,76,55,50,45,65])\n",
    "weights = np.array([25,50,55,75,80,85,50,65,85,55,45,45,50,75,95,65,50,40,45])\n",
    "\n",
    "sns.regplot(heights, weights, color='blue').set_title('Height by Weight');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgLnrK0n9ap4"
   },
   "source": [
    "![Constellations](https://www.explainxkcd.com/wiki/images/9/91/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gy1sSgQU2a4A"
   },
   "source": [
    "## Standard Error of a Coefficient\n",
    "\n",
    "While we can calculate a 95% confidence interval for any estimated parameter, we usually won't refer to the potential spread of parameter estimates by its confidence interval. We'll usually refer to how wide or how narrow the spread is by referring to what's called the \"Standard Error.\" \n",
    "\n",
    "The Standard Error (SE) of a coefficient estimate is the estimated standard deviation of the error in measuring it. So the coefficient itself is the **estimated mean effect** of x on y. and the Standard Error is the **estimated standard deviation** of our coefficient. We use standard errors to calculate the confidence interval. \n",
    "\n",
    "## Standard Error of the Regression\n",
    "\n",
    "The standard error of a coefficient is different from the standard error of the regression. The standard error of the regression as a whole is the average distance that points fall from the regression line. \n",
    "\n",
    "\\begin{align}\n",
    "SE_{est} = \\sqrt{\\frac{\\sum(y_i-\\hat{y})^2}{N}}\n",
    "\\end{align}\n",
    "\n",
    "Does the numerator of that equation look familiar to you? I hope it does by now.\n",
    "\n",
    "Standard Error of the regression as a whole is the average distance that datapoints fall from the regression line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFg2KRXn6I9y"
   },
   "source": [
    "## Precision vs Accuracy\n",
    "\n",
    "![Accuracy vs Precision](https://www.dnasoftware.com/wp-content/uploads/2015/07/targets.png)\n",
    "\n",
    "### Accuracy\n",
    "A regression coefficient that is \"Accurate\" is centered around its \"true\" value. The problem here is that we don't know what the true value actually is, so when we say that a coefficient is more accurate we mean that we suspect that it better represents ground truth.  \n",
    "\n",
    "The more observations we have, the more precise our estimates will be.\n",
    "\n",
    "### Precision\n",
    "A regression coefficient that is \"Precise\" has a small standard error. It has a tighter confidence interval as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6rtwo5e4xSf"
   },
   "source": [
    "# Gauss Markov Assumptions\n",
    "\n",
    "There are five Gauss Markov assumptions (also called conditions) that are required for OLS to be BLUE (the \"Best Linear Unbiased Estimator\"). \n",
    "\n",
    "**0) Well Defined:** $X^{T}X$ is invertible (No perfect multicollinearity), $|X| \\neq 0$\n",
    "\n",
    "**1) Linearity:** the parameters we are estimating using the OLS method must be themselves linear.\n",
    "\n",
    "**2) Random:** our data must have been randomly sampled from the population.\n",
    "\n",
    "**3) Non-Collinearity:** the regressors (x vars) being calculated aren’t perfectly (or highly) correlated with each other.\n",
    "\n",
    "**4) Exogeneity:** the regressors (x vars) aren’t correlated with the error term.\n",
    "\n",
    "- Omitted Variables Bias (Ice Cream Sales and Burglaries)\n",
    "- Instrumental Variables: A regression of education on earnings would be biased both education and ability are both influenced by  influenced by natural ability. We use an additional \"Instrumental Variable\" that is correlated with of schooling and earnings but isn't correlated with ability in order to estimate the effect of years of schooling on earnings. (Month of birth - Angrist and Kreuger) \n",
    "\n",
    "**5) Homoskedasticity:** no matter what the values of our regressors might be, the error of the variance is constant.\n",
    "\n",
    "[Statistics How To - Gauss Markov Assumptions](https://www.statisticshowto.datasciencecentral.com/gauss-markov-theorem-assumptions/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xob1htDnDXDP"
   },
   "source": [
    "# Enough Terminology Zoo, Lets Do Stuff!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5coNfb12EvZ_"
   },
   "source": [
    "# Finding Standard Errors of Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7y-0MORFAE1"
   },
   "source": [
    "Scikit-Learn is built to be a machine learning library, and machine learning typically prioritizes making accurate predictions over interpreting model parameters. Due to this, there aren't any easy ways to calculate standard errors of our coefficients using Sklearn. We'll need to use a different library called **statsmodels**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "colab_type": "code",
    "id": "gxyuM0QUFZ8A",
    "outputId": "963e0007-2cd1-42c9-e43a-4818cbf9ca5d"
   },
   "outputs": [],
   "source": [
    "# Read in dataset \n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv\")\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1058
    },
    "colab_type": "code",
    "id": "p1_VbyFejpFt",
    "outputId": "8e318f9a-e5dc-4cc0-9427-e40a9fd8b44d"
   },
   "outputs": [],
   "source": [
    "# Most homes weren't renovated\n",
    "df['yr_renovated'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "mNQviLq7ihDG",
    "outputId": "f85a275d-5875-433d-95e6-0fea9e808d0d"
   },
   "outputs": [],
   "source": [
    "# Drop columns that I don't care about\n",
    "df = df.drop(columns=['id','date','zipcode','lat','long','yr_renovated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "colab_type": "code",
    "id": "HSMkG5YKLw2b",
    "outputId": "54d958e1-eb45-4d6d-ed20-e80e4aa6097f"
   },
   "outputs": [],
   "source": [
    "# Plot scatterplots\n",
    "target = 'price'\n",
    "features = df.columns.drop(target)\n",
    "for feature in features:\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vyb96G3wF16i"
   },
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "target = 'price'\n",
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', \n",
    "            'floors', 'waterfront', 'view', 'condition', 'grade', \n",
    "            'sqft_above', 'sqft_basement', 'yr_built', \n",
    "            'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "X = df[features] \n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "cGqYZG47GM7p",
    "outputId": "9cf3f23d-34c0-4c9d-8e2a-bc82d522b025"
   },
   "outputs": [],
   "source": [
    "# Use Statsmodels to run a regression\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlGd_1PuxJiO"
   },
   "source": [
    "### Interpretation of P-Value\n",
    "\n",
    "\"The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.\" [Minitab Blog](http://blog.minitab.com/blog/adventures-in-statistics-2/how-to-interpret-regression-analysis-results-p-values-and-coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "y5GOWFu2i77K",
    "outputId": "15fcc790-9f27-4052-940d-2d95e88e5e03"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "_PPWk5t8jMZJ",
    "outputId": "e42ba995-014a-4cdc-d76d-5c8a42cea13e"
   },
   "outputs": [],
   "source": [
    "# Re-run regression without outliers.\n",
    "X = df[features] \n",
    "y = df[target]\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "QhqiHByJL-0_",
    "outputId": "45a71fd9-caf0-4985-8024-b5e871ab4097"
   },
   "outputs": [],
   "source": [
    "df['ln_price'] = np.log(df['price'])\n",
    "df = df.drop(columns='price')\n",
    "\n",
    "target = 'ln_price'\n",
    "features = df.columns.drop(target)\n",
    "for feature in features:\n",
    "    sns.scatterplot(x=feature, y=target, data=df, alpha=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "colab_type": "code",
    "id": "aKjq-EjXhtb_",
    "outputId": "bbcf2b63-77b5-40ac-8c11-700e3e7c1103"
   },
   "outputs": [],
   "source": [
    "# Log-Linear Regression\n",
    "X = df[features] \n",
    "y = df[target]\n",
    "model = sm.OLS(y, sm.add_constant(X))\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ll0UcMOoKW12"
   },
   "source": [
    "[King County](https://www.google.com/maps/place/King+County,+WA/@47.4269284,-122.9244266,8z/data=!3m1!4b1!4m5!3m4!1s0x54905c8c832d7837:0xe280ab6b8b64e03e!8m2!3d47.5480339!4d-121.9836029)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B3Rk0HFIPHJt"
   },
   "source": [
    "# Collinearity/Multicollinearity\n",
    "\n",
    "When two variables are close to being a linear combination of each other we call this **collinearity** or having high levels of collinearity. If there are three of more variables all with significant levels of collinearity we call this \"multicollinearity\" but people basically use the two terms interchangeably. \n",
    "\n",
    "## Perfect Multicollinearity\n",
    "Variables variables are **perfectly** collinear when the vectors that represent them are linearly dependent. This means that if plotted against each other in a scatter plot all of the points would fall on the same line. We mentioned briefly that perfect multicollinearity breaks OLS because it makes it so that the X matrix is not invertible. \n",
    "\n",
    "Perfect multicollinearity usually is caused by careless feature engineering usually through transforming the units of a variable and then keeping both variables in the regression. It can also be created through the one-hot-encoding of binary categorical variables. \n",
    "\n",
    "## Why is Collinearity Bad? \n",
    "\n",
    "High levels of Collinearity in a dataset is bad because it increases standard errors and therefore makes estimates of our coefficients less precise. Very high levels of collinearity (nearing perfect multicollinearity can cause standard errors to grow drastically.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of two collinear features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='sqft_basement', y='sqft_living', data=df, alpha=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for high levels of collinearity\n",
    "\n",
    "We test for high levels of collinearity by calculating the dataset's **Variance Inflation Factor** or VIF. From Wikipedia:\n",
    "\n",
    "> \"In statistics, the variance inflation factor (VIF) is the ratio of variance in a model with multiple terms, divided by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.\" [VIF Wikipedia](https://en.wikipedia.org/wiki/Variance_inflation_factor)\n",
    "\n",
    "As a rule of thumb any variable that has a VIF > 10 needs to be dealt with (probably dropped from your model). If you see a VIF greater than 10 it is likely that two x variables are highly correlated. Remember that we can use the correlation matrix to check levels of correlation between our independent variables.\n",
    "\n",
    "(Ignore the variance inflation factor for the constant. It should be high, even infinite.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "sImOX7_HSYu_",
    "outputId": "8d1eb33e-b4be-4536-da1d-3dc67477d409"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "pd.Series(vif, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude collinear features and refit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ln_price'\n",
    "features = ['bedrooms',\n",
    "            'bathrooms',\n",
    "            'sqft_living',\n",
    "            'sqft_lot',\n",
    "            'floors',\n",
    "            'waterfront',\n",
    "            'view',\n",
    "            'condition',\n",
    "            'grade',\n",
    "            'yr_built',\n",
    "            'sqft_living15',\n",
    "            'sqft_lot15']\n",
    "\n",
    "y = df[target]\n",
    "X = df[features]\n",
    "X = sm.add_constant(X)\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "pd.Series(vif, X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kic8-xfVEr5j"
   },
   "source": [
    "# Homoskedasticity and Heteroskedasticity\n",
    "\n",
    "What a big complicated words. Also, some poeple spell them \"homoscedasticity\" and \"heteroscedasticity\" but that just feels wrong to me somehow.\n",
    "\n",
    "## Homoskedasticity\n",
    "\n",
    "Homoskedasticity means that along our entire domain (x axis) the residuals are about the same distance from our regression line (on average).\n",
    "\n",
    "## Heteroskedasticity.\n",
    "\n",
    "Our data points exhibit heteroskedasticity when they don't exhibit homoskedasticity. This is much easier to explain by just showing a picture.\n",
    "\n",
    "![Homoskedasticity and Heteroskedasticity](https://www.researchgate.net/profile/Akhmad_Fauzy/publication/319091775/figure/fig2/AS:526826008846336@1502616516067/Example-of-homoscedasticity-Ideally-residuals-are-randomly-scattered-around-0-the.png)\n",
    "\n",
    "Looking at scatterplots of our data are there any places where we might be worried about heteroskedasticity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "Z6LKS8RcYBYh",
    "outputId": "643bdb48-111c-479d-9f01-34d2f05ab1ae"
   },
   "outputs": [],
   "source": [
    "target = 'ln_price'\n",
    "features = df.columns.drop(target)\n",
    "for feature in features:\n",
    "    sns.lmplot(x=feature, y=target, data=df, scatter_kws=dict(alpha=0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YD44LDGzYJKh"
   },
   "source": [
    "## Which variables might potentially be offenders? \n",
    "\n",
    "## Addressing Heteroskedasticity\n",
    "\n",
    "If heteroskedasticity exists in our dataset it will damage our standard errors and make our estimates less precise. You have to remember that any challenges that damages the reliability of standard errors also damages the reliability of confidence intervals and hypothesis tests. Therefore, these challenges that damage standard errors also damage a whole host of statistical tools that we would normally like to rely on.\n",
    "\n",
    "Dealing with heteroskedasticity is pretty straightforward, we simply employ what are called \"robust standard errors\" I won't go into depth on how they this works here, but robust standard errors essentially correct heteroskedasticity in our data while the side effects are minimal. Due to this if you are suspicious of heteroskedasticity in your dataset and you intend to interpret the coefficients of your model. You should run the regression using robust standard errors the majority of the time. Lets see how much our regression output changes when we use robust standard errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "vyjwmUT7eW8y",
    "outputId": "13a1f599-ea21-4885-da07-71868696096c"
   },
   "outputs": [],
   "source": [
    "# Let's run our regression again using Robust Standard Errors\n",
    "# cov_type='HC3' parameter to .fit() function\n",
    "\n",
    "# Log-Linear Regression\n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living',\n",
    "       'sqft_lot', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_above', 'sqft_basement', 'yr_built', \n",
    "       'sqft_living15', 'sqft_lot15']] \n",
    "\n",
    "y = df['ln_price']\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yPlv_fHobWN"
   },
   "source": [
    "# Function Form Misspecification\n",
    "\n",
    "Say we wanted to fit a polynomial log-linear model to this data. How might we identify (besides visually) potential candidates for polynomial functions? First off, what does the eyeball test point out might be potential candidates for polynomial forms? Here come scatter plots again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "qI5lfn73qRwM",
    "outputId": "c2b56584-2067-4a8d-b008-657b139986eb"
   },
   "outputs": [],
   "source": [
    "target = 'ln_price'\n",
    "features = df.columns.drop(target)\n",
    "for feature in features:\n",
    "    sns.lmplot(x=feature, y=target, data=df, scatter_kws=dict(alpha=0.1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZYKQ_R_cqgrs"
   },
   "source": [
    "I think sqft_living and sqft_above at a minimum are potential candidates for polynomial terms. I want to remind you what an underfit linear regression looks like: \n",
    "\n",
    "![Underfitting](https://cdn-images-1.medium.com/max/1200/1*2RXJ2O-_c2ukaq5p-WQ9tQ.png)\n",
    "\n",
    "This shows that the residuals of an underfit curved functional form will oscilate from negative residuals, to positive and then back to negative. \n",
    "\n",
    "We might expect the residual plot to look something like this:\n",
    "\n",
    "![Underfit Residual graph](http://www.ryanleeallred.com/wp-content/uploads/2019/01/Underfit-Residual-Graph.png)\n",
    "\n",
    "Truly, any bowing in our residuals is cause for concern. Lets plot the actual distribution of the residual graphs and see if our residuals match our eyeball test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LV_o3f1wEz-h"
   },
   "source": [
    "# Residual Plots\n",
    "\n",
    "Plotting our residuals to see their distribution is an extremely useful model diagnostic technique. Lets get familiar with it. \n",
    "\n",
    "The Seaborn library coming through like a champ, yet again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    sns.residplot(X[feature], y, lowess=True, line_kws=dict(color='r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCD0_leusrHE"
   },
   "source": [
    "From our residual plots, I think we can suspect that sqft_lot sqft_lot15 and yr_built all might be candidates for polynomial forms. Lets generate some squared terms and then re-plot the residuals graphs and see if we get any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOZEUA87tFOj"
   },
   "outputs": [],
   "source": [
    "df['sqft_lot_squared'] = df['sqft_lot']**2\n",
    "df['sqft_lot15_squared'] = df['sqft_lot15']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PZs69dX_u9rL"
   },
   "source": [
    "Lets also create a few features from our eyeball test and we'll see which ones seem to be more statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQbHYTX_u893"
   },
   "outputs": [],
   "source": [
    "df['sqft_living_squared'] = df['sqft_living']**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MjbmXNIxupHQ"
   },
   "source": [
    "Lets add these to our regression and run it again to see if it has any considerable impact on coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "YnRbZDK5to7q",
    "outputId": "525e6aa2-978c-4737-9154-7174b6935595"
   },
   "outputs": [],
   "source": [
    "# log-polynomial? linear regression model with robust standard errors \n",
    "\n",
    "# to use Robust Standard Errors pass:\n",
    "# cov_type='HC3' parameter to .fit() function\n",
    "\n",
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_living_squared',\n",
    "       'sqft_lot', 'sqft_lot_squared', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_basement', 'sqft_living15',\n",
    "       'sqft_lot15', 'sqft_lot15_squared']] \n",
    "\n",
    "y = df['ln_price']\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "colab_type": "code",
    "id": "aytEwuPyW2SO",
    "outputId": "93281b90-1cad-4731-899f-3aab2c21f283"
   },
   "outputs": [],
   "source": [
    "X = df[['bedrooms', 'bathrooms', 'sqft_living', 'sqft_living_squared',\n",
    "       'sqft_lot', 'sqft_lot_squared', 'floors', 'waterfront', 'view', 'condition', 'grade',\n",
    "       'sqft_basement', 'sqft_living15',\n",
    "       'sqft_lot15']]\n",
    "\n",
    "y = df['ln_price']\n",
    "\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit(cov_type='HC3')\n",
    "print(results.summary())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model Diagnostics.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
